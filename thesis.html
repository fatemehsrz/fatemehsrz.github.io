
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<script src="https://kit.fontawesome.com/a076d05399.js"></script>

<head>
<title>Fatemeh's PhD Thesis</title>
<meta name="generator" content="Nested http://nestededitor.sourceforge.net/">
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
    menuSettings: {zoom: "Double-Click", zscale: "300%"},
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]},
    MathMenu: {showRenderer: false},
    "HTML-CSS": {
        availableFonts: ["TeX"],
        preferredFont: "TeX",
        imageFont: null
    }
  });
</script>
<style type="text/css">
    body { background-color: White; font-family: Helvetica, Futura, "Trebuchet MS", sans-serif; width:900px; margin:0 auto;}
    h1 { color: black; font-family: Helvetica, Futura, "Trebuchet MS", sans-serif; }
    p { color: black; font-family: Helvetica, Futura, "Trebuchet MS", sans-serif;}
</style>
</head>

<br/>
<div id="header" class="header" align="center">
<h1>Semantic Scene Segmentation with Minimal Labeling Effort</h1>

<table style="width:100%">
  <tr>
    <td style="text-align:center"><font size="4"><a href="https://fatemeh-slh.github.io/">Fatemeh Sadat Saleh</a></font></td>
  </tr>
   <tr>
    <td style="text-align:center"><font size="4">PhD Thesis, Australian National University, 2020</font></td>
  </tr>
</table>


</div>

<h2>Abstract</h2>
Semantic scene segmentation - the process of assigning a semantic label to every pixel in an input image - is an important task in computer vision where an autonomous system or a robot needs to differentiate between different parts of the scene/objects and recognize the class of each one for adequate physical interactions. The most successful methods that try to solve this problem are fully-supervised approaches based on Convolutional Neural Networks (CNNs). Unfortunately, these methods require large amounts of training images with pixel-level annotations, which are expensive and time-consuming to obtain. In this thesis, we aim to alleviate the manual effort of annotating real images by designing either weakly-supervised learning strategies that can leverage image-level annotations, such as image tags, which are cheaper to obtain, or effective ways to exploit synthetic data which can be labeled automatically. In particular, we make several contributions to the literature of semantic scene segmentation with minimal labeling effort. Firstly, we introduce a novel weakly-supervised semantic segmentation technique to address the problem of semantic scene segmentation with one of the minimal level of human supervision, image-level tags, which simply determines present and absent classes within an image. The proposed method is able to extract markedly accurate foreground/background masks from the pre-trained network itself, forgoing external objectness modules or using pixel-level/bounding box annotations, and use them as priors in an appropriate loss function. Secondly, we improve the performance of this framework by extracting class-specific foreground masks instead of a single generic foreground mask, with virtually no additional annotation cost. Thirdly, we found that a general limitation of existing tag-based semantic segmentation techniques is the assumption of having just one background class in the scene, which, by relying on the object recognition pre-trained networks or objectness modules, restricts the applicability of these methods to segmenting foreground objects only. However, in practical applications, such as autonomous navigation, it is often crucial to reason about multiple background classes. Thus, in this thesis, we introduce a weakly-supervised video semantic segmentation method in which there are multiple foreground and multiple background classes in the scene. To this end, we propose an approach to doing so by making use of classifier heatmaps. Then, we develop a two-stream deep architecture that can jointly leverage appearance and motion, and we design a loss based on the heatmaps to train this network. In the last contribution, we propose a novel technique for using synthetic data which lets us perform semantic segmentation without having any manual annotation, not even image-level tags. Although there exist approaches that utilize synthetic data, we use a drastically different way to handle synthetic images that does not require seeing any real images during training time. This approach builds on the observation that foreground and background classes are not affected in the same manner by the domain shift, and thus should be treated differently. All the methods introduced in this thesis are evaluated on standard semantic segmentation datasets consisting of single background and multiple background scenes. The experiments of each chapter provide compelling evidence that all of our approaches are more efficient than the contemporary baselines. All in all, semantic scene segmentation methods with minimal labeling effort, such as those in this thesis, are crucial for having less expensive annotation processes in terms of time and money. Moreover, this will make large-scale semantic segmentation much more practical than the current models relying on full supervision, as well as lead to solutions that generalize much better than existing ones, thanks to the use of images depicting a great diversity of scenes.
<br>


<hr class="heavy">

<br/><br/>
<p align="center">
  <a href="https://github.com/mix-and-match/mix-and-match-tutorial" style="font-size: 20px;">[ANU repository]</a>
  <a href="https://openresearch-repository.anu.edu.au/bitstream/1885/189389/1/Saleh%20Thesis%202020.pdf" style="font-size: 20px;">[PDF]</a>
</p>



<h2 id="bibtex">Cite</h2>
If you find this thesis useful in your own research, please consider citing:
<pre>
@phdthesis{1885-189389,
	author = {Saleh, Fatemehsadat},
	title = {Semantic Scene Segmentation with Minimal Labeling Effort},
	school = {College of Engineering and Computer Science, The Australian National University},
	year = {2020}
}
</pre>
